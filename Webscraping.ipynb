{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6YY9_VPFBFfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20c6aba9-a192-44df-da8e-c8675c23bcea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping completed!\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import json\n",
        "\n",
        "# Function to scrape data from a single URL\n",
        "def scrape_url(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Extract relevant information from the webpage\n",
        "        title = soup.title.string\n",
        "        text = soup.get_text()\n",
        "        links = [link.get('href') for link in soup.find_all('a', href=True)]\n",
        "        images = [img.get('src') for img in soup.find_all('img', src=True)]\n",
        "\n",
        "        return {'title': title, 'text': text, 'links': links, 'images': images}\n",
        "    except Exception as e:\n",
        "        print(\"Error scraping:\", url, e)\n",
        "        return None\n",
        "\n",
        "# List of URLs to scrape\n",
        "urls = [\n",
        "    \"https://products.basf.com/global/en/ci/n-vinyl-2-pyrrolidone.html\",\n",
        "    \"https://pubchem.ncbi.nlm.nih.gov/compound/N-Vinyl-2-pyrrolidone\",\n",
        "    \"https://www.shokubai.co.jp/en/products/detail/nvp/\",\n",
        "    \"https://pubchem.ncbi.nlm.nih.gov/compound/N-Vinyl-2-pyrrolidone\",\n",
        "    \"https://www.sciencedirect.com/topics/pharmacology-toxicology-and-pharmaceutical-science/1-vinyl-2-pyrrolidinone\",\n",
        "    \"https://www.ncbi.nlm.nih.gov/books/NBK498761/#:~:text=It%20is%20used%20in%20the,the%20synthesis%20of%20phenolic%20resins\",\n",
        "    \"https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/polyvinylpyrrolidone#:~:text=PVP%20added%20to%20iodine%20forms,trade%20name%20Betadine%20and%20Pyodine\",\n",
        "    \"https://www.shokubai.co.jp/en/products/detail/nvp/#:~:text=N%2Dvinylpyrrolidone%20is%20a%20nonionic,monomer%20with%20the%20following%20features.&text=N%2Dvinylpyrrolidone%20is%20used%20as,of%20reactivity%20with%20UV%20irradiation\",\n",
        "    \"https://adhesives.specialchem.com/product/m-basf-n-vinyl-pyrrolidone-nvp\",\n",
        "    \"https://www.science.gov/topicpages/n/n-vinyl+pyrrolidone+nvp\",\n",
        "    \"https://shdexiang.en.made-in-china.com/product/tXfQDioPsKVn/China-N-Vinylpyrrolidone-CAS-No-88-12-0-C6h9no.html\",\n",
        "    \"https://www.cphi-online.com/nvp-n-vinylpyrrolidone-prod1288298.html\",\n",
        "    \"https://www.mdpi.com/2073-4360/11/6/1079\",\n",
        "    # this all URLs\n",
        "]\n",
        "\n",
        "# Scraping each URL and storing the data\n",
        "scraped_data = []\n",
        "for url in urls:\n",
        "    data = scrape_url(url)\n",
        "    if data:\n",
        "        scraped_data.append(data)\n",
        "\n",
        "# Storing the scraped data in JSON format\n",
        "with open('scraped_data.json', 'w') as outfile:\n",
        "    json.dump(scraped_data, outfile, indent=4)\n",
        "\n",
        "# Storing the scraped data in CSV format\n",
        "with open('scraped_data.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    fieldnames = ['title', 'text', 'links', 'images']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    for data in scraped_data:\n",
        "        writer.writerow(data)\n",
        "\n",
        "print(\"Scraping completed!\")\n"
      ]
    }
  ]
}